{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoFfflYipSeC"
   },
   "outputs": [],
   "source": [
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sPEP5kyA3Qmp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, random\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython import display\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision.models as models\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall, AUROC, ConfusionMatrix\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgokW6UvkXox"
   },
   "source": [
    "### Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eJ9Otum7vtM4"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# Путь к таблице\n",
    "PATH_TO_TABLE = 'data_fullness.xlsx'\n",
    "data = pd.read_excel(PATH_TO_TABLE, header=0)\n",
    "\n",
    "data = data.drop(columns=['Дата вывоза', 'Общий путь к фото', \n",
    "                          'Относительный путь к фото', 'Ответственный'], errors='ignore')\n",
    "data = data.drop(index=[35686, 35687], errors='ignore')\n",
    "data['Путь к фото контейнера'] = data.apply(lambda x: x['Путь к фото контейнера'].split('/')[-1], axis=1)\n",
    "\n",
    "# Путь к папке с файлами\n",
    "ROOT = 'data/'\n",
    "inaccessible_indices = []\n",
    "for index, row in data.iterrows():\n",
    "    path = row['Путь к фото контейнера']\n",
    "    if os.path.isfile(ROOT + path) is False:\n",
    "        inaccessible_indices.append(index)\n",
    "\n",
    "data = data.drop(index=inaccessible_indices, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0fgkeMvDkXoz",
    "outputId": "b6d67039-33df-4f42-c784-a83843532a25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Процент наполнения\n",
       "0     2346\n",
       "1     3554\n",
       "2     3806\n",
       "3    16569\n",
       "4     9398\n",
       "Name: Контейнерная площадка, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def match_weight_5(row):\n",
    "    weight = row['Процент наполнения']\n",
    "    if weight <= 25:\n",
    "        return 0 #20\n",
    "    elif 25 < weight <= 50:\n",
    "        return 1 #40\n",
    "    elif 50 < weight <= 75:\n",
    "        return 2 #60\n",
    "    elif 75 < weight <= 100:\n",
    "        return 3 #100\n",
    "    elif weight > 100:\n",
    "        return 4 #140\n",
    "\n",
    "data['Процент наполнения'] = data.apply(match_weight_5, axis=1)\n",
    "data.groupby(by='Процент наполнения').count()['Контейнерная площадка'].plot(kind='bar')\n",
    "data.groupby(by='Процент наполнения').count()['Контейнерная площадка']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_Gw3BUlm6AF"
   },
   "source": [
    "width $\\in$ [300,1632], height $\\in$ [300,1632]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T4Taun7qac0"
   },
   "source": [
    "### Предобработка датасета, создание даталоадера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8i5uNqpMkXo1"
   },
   "outputs": [],
   "source": [
    "class PolygonWeightDataset(Dataset):\n",
    "    \"\"\"Custom Polygon Dataset dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, root_dir, return_name=False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame):\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.return_name = return_name \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir, self.data.loc[idx,'Путь к фото контейнера'])\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "\n",
    "        label = torch.tensor(float(self.data.loc[idx,'Процент наполнения']))\n",
    "        label = label.long()      \n",
    "        \n",
    "        if self.return_name:\n",
    "            return image, label, img_name\n",
    "        else:\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1uUTml0bMwgt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.15, random_state=24) # 0.15 for test set\n",
    "data_train, data_val = train_test_split(data_train, test_size=0.18, random_state=24) # 0.18 x 0.85 = 0.153 for vaidation set\n",
    "\n",
    "data_train = data_train.reset_index(drop=True)\n",
    "data_val = data_val.reset_index(drop=True)\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.RandomResizedCrop(224, 224, scale=(0.6, 1.0)),\n",
    "    A.Normalize(max_pixel_value=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# скорее всего, можно подобрать набор аугментаций лучше, чем этот\n",
    "train_transform = A.Compose([\n",
    "        A.RandomRotate90(),\n",
    "        A.Flip(),\n",
    "        A.Transpose(),\n",
    "        A.GaussNoise(p=1.0),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=.1),\n",
    "            A.PiecewiseAffine(p=0.3),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.Sharpen(),\n",
    "            A.Emboss(),\n",
    "            A.RandomBrightnessContrast(),            \n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        A.RandomResizedCrop(224, 224, scale=(0.6, 1.0)),\n",
    "        A.Normalize(max_pixel_value=1.0),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "train_dataset = PolygonWeightDataset(data_train, ROOT, transform=train_transform)\n",
    "val_dataset = PolygonWeightDataset(data_val, ROOT, transform=val_transform)\n",
    "test_dataset = PolygonWeightDataset(data_test, ROOT, transform=val_transform)\n",
    "\n",
    "batch_size = 60\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=32, \n",
    "                              pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False, \n",
    "                            num_workers=32, \n",
    "                            pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=False, \n",
    "                             num_workers=32, \n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9zMslKkkXo2"
   },
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hnT2NlEXkXo2"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "class MobileNetv3_WeightDefiner(pl.LightningModule):\n",
    "    def __init__(self, num_features_fc=1280, num_classes=7, lr=1e-3, freeze=True):\n",
    "        super(MobileNetv3_WeightDefiner, self).__init__()\n",
    "        \n",
    "        # Architecture\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        mobilenet = models.mobilenet_v3_large(pretrained=True)\n",
    "        mobilenet._modules.pop('classifier')\n",
    "        self.mobilenet_features = nn.Sequential(mobilenet._modules)\n",
    "        mobilenet = None\n",
    "        \n",
    "        if freeze:\n",
    "            for name, param in self.mobilenet_features.named_parameters():\n",
    "                param.requires_grad = False     \n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=960, out_features=num_features_fc, bias=True),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.35),\n",
    "            nn.Linear(num_features_fc, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialise loss\n",
    "        self.loss = nn.CrossEntropyLoss(weight=torch.Tensor([3.0451929 , 1.99710843, 1.89151769, 0.42928177, 0.76176471]))\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_precision = Precision(num_classes=num_classes, average=None)\n",
    "        self.train_recall = Recall(num_classes=num_classes, average=None)\n",
    "        self.train_mean_acc = Accuracy()\n",
    "        \n",
    "        self.valid_conf_matrix = ConfusionMatrix(num_classes=num_classes)\n",
    "        self.valid_precision = Precision(num_classes=num_classes, average=None)\n",
    "        self.valid_recall = Recall(num_classes=num_classes, average=None)\n",
    "        self.valid_mean_acc = Accuracy()\n",
    "        \n",
    "        # save hyper-parameters to self.hparams (auto-logged by W&B)\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mobilenet_features(x) # out: [batch_size, 1280, 1, 1]\n",
    "        x = torch.flatten(x, 1) # out: [batch_size, 1280]\n",
    "        x = self.linear_layers(x) # out: [batch_size, num_classes]\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.loss(logits, labels)\n",
    "        \n",
    "        # logs metrics for each training_step and the average across the epoch (to the progress bar and logger)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.train_precision(logits, labels)\n",
    "        self.train_recall(logits, labels)\n",
    "        self.train_mean_acc(logits, labels)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, outs):\n",
    "        train_precision = self.train_precision.compute()\n",
    "        train_recall = self.train_recall.compute()\n",
    "        train_mean_acc = self.train_mean_acc.compute()\n",
    "        for i in range(self.num_classes):\n",
    "            self.log(f'train_recall_{i}', train_recall[i])\n",
    "            self.log(f'train_precision_{i}', train_precision[i])\n",
    "        self.log('train_acc', train_mean_acc)\n",
    "\n",
    "\n",
    "            \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.loss(logits, labels)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.valid_conf_matrix(logits, labels)\n",
    "        self.valid_precision(logits, labels)\n",
    "        self.valid_recall(logits, labels)\n",
    "        self.valid_mean_acc(logits, labels)\n",
    "\n",
    "        \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        valid_conf_matrix = self.valid_conf_matrix.compute().detach().cpu().numpy().astype(int)\n",
    "        valid_precision = self.valid_precision.compute()\n",
    "        valid_recall = self.valid_recall.compute()\n",
    "        valid_mean_acc = self.valid_mean_acc.compute()\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            self.log(f'valid_recall_{i}', valid_recall[i])\n",
    "            self.log(f'valid_precision_{i}', valid_precision[i])\n",
    "        self.log('valid_acc', valid_mean_acc)\n",
    "        \n",
    "        df_cm = pd.DataFrame(\n",
    "            valid_conf_matrix,\n",
    "            index=['true <25', 'true <50', 'true <75', 'true <100', 'true >100'], \n",
    "            columns=['<25', '<50', '<75', '<100', '>100'], \n",
    "            ) \n",
    "        plt.figure(figsize = (10,7))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='d', cmap='Spectral').get_figure()\n",
    "        plt.close(fig_)\n",
    "        wandb.log({\"confusion_matrix\": [wandb.Image(fig_, caption=\"Confusion matrix\")]})\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=40, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_ErjoA4qM7d"
   },
   "source": [
    "### Обучение сетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "0412270e95d84d72857554d6f862fb49",
      ""
     ]
    },
    "id": "AwOAWpGlkXo3",
    "outputId": "b9f0d027-82dd-4d61-f472-a6ffc08fe3aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnali\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maxim/Загрузки/Alina_Big3/wandb/run-20220319_110939-2yt5ayh0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nali/big3_weight/runs/2yt5ayh0\" target=\"_blank\">ethereal-blaze-58</a></strong> to <a href=\"https://wandb.ai/nali/big3_weight\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name               | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0 | mobilenet_features | Sequential       | 3.0 M \n",
      "1 | linear_layers      | Sequential       | 1.2 M \n",
      "2 | loss               | CrossEntropyLoss | 0     \n",
      "3 | train_precision    | Precision        | 0     \n",
      "4 | train_recall       | Recall           | 0     \n",
      "5 | train_mean_acc     | Accuracy         | 0     \n",
      "6 | valid_conf_matrix  | ConfusionMatrix  | 0     \n",
      "7 | valid_precision    | Precision        | 0     \n",
      "8 | valid_recall       | Recall           | 0     \n",
      "9 | valid_mean_acc     | Accuracy         | 0     \n",
      "--------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "3.0 M     Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.834    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f3f3fee8154ea99ea2ff18029d431e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='25.723 MB of 25.723 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>train_precision_0</td><td>▁</td></tr><tr><td>train_precision_1</td><td>▁</td></tr><tr><td>train_precision_2</td><td>▁</td></tr><tr><td>train_precision_3</td><td>▁</td></tr><tr><td>train_precision_4</td><td>▁</td></tr><tr><td>train_recall_0</td><td>▁</td></tr><tr><td>train_recall_1</td><td>▁</td></tr><tr><td>train_recall_2</td><td>▁</td></tr><tr><td>train_recall_3</td><td>▁</td></tr><tr><td>train_recall_4</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▄▅▅▆▇▇▇▇█</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>valid_acc</td><td>▁</td></tr><tr><td>valid_precision_0</td><td>▁</td></tr><tr><td>valid_precision_1</td><td>▁</td></tr><tr><td>valid_precision_2</td><td>▁</td></tr><tr><td>valid_precision_3</td><td>▁</td></tr><tr><td>valid_precision_4</td><td>▁</td></tr><tr><td>valid_recall_0</td><td>▁</td></tr><tr><td>valid_recall_1</td><td>▁</td></tr><tr><td>valid_recall_2</td><td>▁</td></tr><tr><td>valid_recall_3</td><td>▁</td></tr><tr><td>valid_recall_4</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>lr-Adam</td><td>0.004</td></tr><tr><td>train_acc</td><td>0.30683</td></tr><tr><td>train_loss</td><td>1.49397</td></tr><tr><td>train_precision_0</td><td>0.20074</td></tr><tr><td>train_precision_1</td><td>0.15905</td></tr><tr><td>train_precision_2</td><td>0.14152</td></tr><tr><td>train_precision_3</td><td>0.5196</td></tr><tr><td>train_precision_4</td><td>0.37195</td></tr><tr><td>train_recall_0</td><td>0.50092</td></tr><tr><td>train_recall_1</td><td>0.22088</td></tr><tr><td>train_recall_2</td><td>0.20692</td></tr><tr><td>train_recall_3</td><td>0.21279</td></tr><tr><td>train_recall_4</td><td>0.49816</td></tr><tr><td>trainer/global_step</td><td>449</td></tr><tr><td>val_loss</td><td>1.35235</td></tr><tr><td>valid_acc</td><td>0.32759</td></tr><tr><td>valid_precision_0</td><td>0.2115</td></tr><tr><td>valid_precision_1</td><td>0.2061</td></tr><tr><td>valid_precision_2</td><td>0.12403</td></tr><tr><td>valid_precision_3</td><td>0.58524</td></tr><tr><td>valid_precision_4</td><td>0.38026</td></tr><tr><td>valid_recall_0</td><td>0.6351</td></tr><tr><td>valid_recall_1</td><td>0.26843</td></tr><tr><td>valid_recall_2</td><td>0.05387</td></tr><tr><td>valid_recall_3</td><td>0.08949</td></tr><tr><td>valid_recall_4</td><td>0.82219</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ethereal-blaze-58</strong>: <a href=\"https://wandb.ai/nali/big3_weight/runs/2yt5ayh0\" target=\"_blank\">https://wandb.ai/nali/big3_weight/runs/2yt5ayh0</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220319_110939-2yt5ayh0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "# seed_everything(42, workers=True)\n",
    "\n",
    "# init model\n",
    "weight_definer = MobileNetv3_WeightDefiner(num_classes=5, lr=0.004)\n",
    "\n",
    "# Wandb logging\n",
    "wandb_logger = WandbLogger(project='big3_weight', entity='nali', log_model='all') # здесь надо будет заменить название проекта и ник\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"valid_acc\",\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"5cl-best-acc-{epoch:02d}-{valid_acc:.2f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "# training the model\n",
    "trainer = pl.Trainer(deterministic=False,\n",
    "                     gpus=1, \n",
    "                     logger=wandb_logger, \n",
    "                     max_epochs=100,\n",
    "                     callbacks=lr_monitor, # надо заменить на callbacks=[checkpoint_callback, lr_monitor], если нужно сохранить веса\n",
    "                     num_sanity_val_steps=0,\n",
    "                    )\n",
    "\n",
    "\n",
    "trainer.fit(weight_definer, train_dataloader, val_dataloader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVdmz5Ozo751"
   },
   "source": [
    "### Подбор значения learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7maoRhfkXo3"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "# init model\n",
    "weight_definer = MobileNetv3_WeightDefiner(num_classes=5, lr=1e-3)\n",
    "\n",
    "\n",
    "# training the model\n",
    "trainer = pl.Trainer(deterministic=True,\n",
    "                     auto_lr_find=True,\n",
    "                     gpus=1, \n",
    "                     max_epochs=100,\n",
    "                     num_sanity_val_steps=0,\n",
    "                     stochastic_weight_avg=True)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tuner.lr_find(weight_definer, train_dataloader, val_dataloader)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "print(lr_finder.suggestion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQcPe4kUkXo4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z578ZqBiMG8t"
   },
   "source": [
    "# To-do:\n",
    "* проверить все данные на наличие битых картинок: влияют ли они на итоговые метрики?\n",
    "* добавить реализацию поэтапного обучения: сначала только последние слои и затем следующим этапом обучение всей сети (разморозка нескольких слоев) - https://pytorch-lightning.readthedocs.io/en/stable/starter/introduction_guide.html#transfer-learning\n",
    "* сделать поиск по гиперпараметрам\n",
    "* проверить работу андерсэмплинга (уменьшить 3ий класс)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "PgokW6UvkXox",
    "_T4Taun7qac0",
    "f9zMslKkkXo2",
    "1_ErjoA4qM7d",
    "eVdmz5Ozo751"
   ],
   "machine_shape": "hm",
   "name": "Weight_lightning_edited.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
